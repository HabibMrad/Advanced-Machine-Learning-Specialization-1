# Advanced Machine Learning Specialization

**About this Specialization** This specialization gives an introduction to deep learning, reinforcement learning, natural language understanding, computer vision and Bayesian methods. Top Kaggle machine learning practitioners and CERN scientists will share their experience of solving real-world problems and help you to fill the gaps between theory and practice. Upon completion of 7 courses you will be able to apply modern machine learning methods in enterprise and understand the caveats of real-world data and settings.



# Introduction to Deep Learning

## About this Course
The goal of this course is to give learners basic understanding of modern neural networks and their applications in computer vision and natural language understanding. The course starts with a recap of linear models and discussion of stochastic optimization methods that are crucial for training deep neural networks. Learners will study all popular building blocks of neural networks including fully connected layers, convolutional and recurrent layers. 
Learners will use these building blocks to define complex modern architectures in TensorFlow and Keras frameworks. In the course project learner will implement deep neural network for the task of image captioning which solves the problem of giving a text description for an input image.

The prerequisites for this course are: 

- Basic knowledge of Python.
- Basic linear algebra and probability.

Please note that this is an advanced course and we assume basic knowledge of machine learning. You should understand:

- Linear regression: mean squared error, analytical solution.
- Logistic regression: model, cross-entropy loss, class probability estimation.
- Gradient descent for linear models. Derivatives of MSE and cross-entropy loss functions.
- The problem of overfitting.
- Regularization for linear models.


## Resources
- [Course Resouces](./resources)


## My Course Notes

### Week1
- Overview
    - Linear Regression
    - Linear Model for Classification
    - Gradient Descent
    - Overfitting
    - Regularization
    - Stochastic Gradient Descent
- [Note](./week1/README.md)
- [Slides](./week1/slides)
- [Projects](./week1/projects)
    - [week01_pa.ipynb](./week1/projects/week01_pa.ipynb)


### Week2
- Overview
    - Backpropogation
    - Chain Rule
    - Matrix Derivatives
    - Multi-layer Perceptron
- [Note](./week2/README.md)
- [Slides](./week2/slides)
- [Projects](./week2/projects)
    - [NumpyNN_honor.ipynb](./week2/projects/NumpyNN_honor.ipynb)
    - [tensorflow_examples_from_video.ipynb](./week2/projects/v2/tensorflow_examples_from_video.ipynb)
    - [digits_classification.ipynb](./week2/projects/v2/digits_classification.ipynb)
    - [intro_to_tensorflow.ipynb](./week2/projects/v2/intro_to_tensorflow.ipynb)
    - [mnist_with_keras.ipynb](./week2/projects/v2/mnist_with_keras.ipynb)


### Week3
- Overview
    - Convolution
    - Pooling
    - Tricks for DNN
    - Modern Architecture CNN
    - Transfer Learning
    - Other CV Problems
- [Note](./week3/README.md)
- [Slides](./week3/slides)
- [Projects](./week3/projects)
    - [week3_task1_first_cnn_cifar10_clean.ipynb](./week3/projects/week3_task1_first_cnn_cifar10_clean.ipynb)
    - [week3_task2_fine_tuning_clean.ipynb](./week3/projects/week3_task2_fine_tuning_clean.ipynb)



### Week4
- Overview
    - Unsupervised Learning
    - Autoencoder
    - Word Embedding
    - Generative Models
    - Generative Adversatial Networks
- [Note](./week4/README.md)
- [Slides](./week4/slides)
- [Projects](./week4/projects)
    - [Autoencoders-task.ipynb](./week4/projects/Autoencoders-task.ipynb)
    - [Adversarial-task.ipynb](./week4/projects/Adversarial-task.ipynb)



### Week5
- Overview
    - Deep Learning for Sequential Data
    - Simple RNN
    - Exploding and Vanishing Gradients Problems
    - LSTM and GRU
    - Some Pratical Use Cases
- [Note](./week5/README.md)
- [Slides](./week5/slides)
- [Projects](./week5/projects)
    - [POS-task.ipynb](./week5/projects/POS-task.ipynb)
    - [RNN-task.ipynb](./week5/projects/RNN-task.ipynb)


### Week6
- Overview
    - Final Project
- [Note](./week6/README.md)
- [Projects](./week6/projects)
    - [week6_final_project_image_captioning_clean.ipynb](./week6/projects/week6_final_project_image_captioning_clean.ipynb)



# How to Win a Data Science Competition: Learn from Top Kagglers

## About this Course
If you want to break into competitive data science, then this course is for you! Participating in predictive modelling competitions can help you gain practical experience, improve and harness your data modelling skills in various domains such as credit, insurance, marketing, natural language processing, salesâ€™ forecasting and computer vision to name a few. At the same time you get to do it in a competitive context against thousands of participants where each one tries to build the most predictive algorithm. Pushing each other to the limit can result in better performance and smaller prediction errors. Being able to achieve high ranks consistently can help you accelerate your career in data science.

In this course, you will learn to analyse and solve competitively such predictive modelling tasks. 

When you finish this class, you will:

- Understand how to solve predictive modelling competitions efficiently and learn which of the skills obtained can be applicable to real-world tasks.
- Learn how to preprocess the data and generate new features from various sources such as text and images.
- Be taught advanced feature engineering techniques like generating mean-encodings, using aggregated statistical measures or finding nearest neighbors as a means to improve your predictions.
- Be able to form reliable cross validation methodologies that help you benchmark your solutions and avoid overfitting or underfitting when tested with unobserved (test) data. 
- Gain experience of analysing and interpreting the data. You will become aware of inconsistencies, high noise levels, errors and other data-related issues such as leakages and you will learn how to overcome them. 
- Acquire knowledge of different algorithms and learn how to efficiently tune their hyperparameters and achieve top performance. 
- Master the art of combining different machine learning models and learn how to ensemble. 
- Get exposed to past (winning) solutions and codes and learn how to read them.

Disclaimer : This is not a machine learning course in the general sense. This course will teach you how to get high-rank solutions against thousands of competitors with focus on practical usage of machine learning methods rather than the theoretical underpinnings behind them.

Prerequisites: 
- Python: work with DataFrames in pandas, plot figures in matplotlib, import and train models from scikit-learn, XGBoost, LightGBM.
- Machine Learning: basic understanding of linear models, K-NN, random forest, gradient boosting and neural networks.


## Resources
- [Course Resouces](./resources)


## My Course Notes

### Week7
- Overview
- [Note](./week7/README.md)
- [Slides](./week7/slides)
- [Projects](./week7/projects)
    - [GBM_drop_tree.ipynb](./week7/projects/GBM_drop_tree.ipynb)
    - [PandasBasics.ipynb](./week7/projects/PandasBasics.ipynb)
- [Quiz](./week8/quiz)

### Week8
- Overview
- [Note](./week8/README.md)
- [Slides](./week8/slides)
- [Projects](./week8/projects)
    - [Building_intuition.ipynb](./week8/projects/Building_intuition.ipynb)
    - [Exploring_anonymized_data.ipynb](./week8/projects/Exploring_anonymized_data.ipynb)
- [Quiz](./week8/quiz)


### Week9
- Overview
- [Note](./week9/README.md)
- [Slides](./week9/slides)
- [Projects](./week9/projects)
- [Quiz](./week8/quiz)



### Week10
- Overview
- [Note](./week10/README.md)
- [Slides](./week10/slides)
- [Projects](./week10/projects)
- [Quiz](./week8/quiz)


### Week11
- Overview
- [Note](./week11/README.md)
- [Slides](./week11/slides)
- [Projects](./week11/projects)
- [Quiz](./week8/quiz)

### Week12
- Overview
- [Note](./week12/README.md)
- [Projects](./week12/projects)
- [Quiz](./week8/quiz)






